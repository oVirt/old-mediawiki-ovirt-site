== Summary ==

This page gather the design of the VM startup revamp. The code which handles the VM startup in current (<= 4.14.x) has become tangled and hard to follow.
A rewrite will be beneficial. Performance improvements about the VDSM startup are covered in a [[VDSM_libvirt_performance_scalability|separate page]]

This page aims to document the internals and the execution flow of a VM startup/migration (because the migration flow is tightly correlated with VM creation).
If you look for a gentler introduction to VDSM architecture, you'd probably better served by other wiki page. The remainder of this page will provide minimal
context and minimal documentation about some other important parts of VDSM like the client interface, the API abstraction, the libvirt interaction and so on.

==== WARNING! ====

This document is work in progress and requently updated, for content and for style/consistency/readability. Proof-reading is also in prgress.

=== Owner ===

* Name: [[User:Fromani|Francesco Romani]]
* Email: [mailto:fromani@redhat.com fromani@redhat.com]
* PM Requirements : N/A
* Email: N/A

=== Current status ===
* Target Release: oVirt 3.5 and following
* Status: Draft/Discussion
* Last updated: 2014-03-05

== Summary of the status quo ==

=== Introduction ===

Inside VDSM, a VM object encapsulate all the data and methods needed to fullfill the oVirt engine requests and commands, to keep track of the VM status (e.g. resource accounting) and
to interact with the hypervisor, theough libvirt.

The creation of a VM object may be the result of different actions, all of which has the purpose to bring up and let the engine manage a virtual machine.
Different actions demands for different code flows. The code flow that will lead to the creation of a VM are

* creation: the most simple flow. A new VM is booted up and brought to existence in a given hosts, while it was previously down on the data center.
* recovering: VDSM resyncs its internal state with libvirt, and retake ownership of the VMs found running in a given host.
* dehibernation: VDSM restores a VM which was hibernated, or from a checkpoint being saved in the past.
* migration destination: used internally, not directly exposed to users. VDSM create a VM to host the result of a migration of a VM from its source node.


=== Implementation ===

All the code which implements the vm creation flow is found in ''vdsm/vm.py''. Code is referenced "like this" in the remainder of this page.

VM objects are created each in its independent thread, to make the caller not-blocking. Each VM objects has its own ''_creationThread'' member (set in te constructor) which runs the ''_startUnderlyingVm'' method which actually implements the VM creation. Note that all the creation flow are intermixed here, and the code is branchy and scatthered through various
helper methods. When ''_startUnderlyingVm'' ends its job, it sets the VM ''_lastStatus'' either to "Up" or "Down".

Note that VM objects are registered inside ''vmContainer'' before the creation process starts, so they are exposed to while the actual creation is still in progress.
For the migration flow, which uses more threads and background operation through libvirt/qemu, synchronization is achieved using ''threading.Event''s, 
which are triggered after certain phases of the creation have been reached.

The synchronization with the engine is regulated by te VM status parameter, which is in turn the result of the aggregation of various fields (see: ''_getStatsInternal'', ''status'', ''_get_lastStatus'', ''_set_lastStatus'')

* the internal VM status field ''_lastStatus''
* a boolean flag reporting if the guest CPU is running or paused ''_guestCpuRunning''
* the status of the guest agent ''_guestEvent'' (note this is '''NOT''' a ''threading.Event'')
* the reported responsiveness of the hypervisor ''_monitorResponse''

A VM objects may receive method invocations while the creation process is still ongoing (including, but not limited to ''getStats'' calls)

pseudo-code-summary of the ''_startUnderlyingVm'' workhorse
        
    def _startUnderlyingVm(self):
        try:
            with throttle(libvirt)
                try:
                    self._run()
                except Exception:
                    handleExceptions()
            if ('migrationDest' in self.conf or 'restoreState' in self.conf) \
                    and self.lastStatus != 'Down':
                self._waitForIncomingMigrationFinish()
            self.lastStatus = 'Up'
            self.saveState()
        except Exception as e:
             handleExceptions()
    
Please note this snippet is '''just pseudo-code stripped from important parts to unclutter the example and highlight the point below''''.
Important parts omitted are: status handling, exception handling, pause code handling).

What we aim to highlight here is the creation flow is scattered through methods at various nesting levels: ''_startUnderlyingVm'' itself, ''_run'', ''_waitForIncomingMigrationFinish'', and the logic
to distinguish among creation flow is scattered as well.

The ''_startUnderlyingVm'' method does some generic preparation for the startup
* sets the commited memory (stripped in the example)
* sets VM internal status (stripped in the example)
* handles the exception/failures triggered by helper methods and takes corrective action
* handles the VM pause reason (stripped in the example)
* saves the VM state for future recovery

The ''_run'' method implements most of the remaining setup common to all the migration flows, and the the '''creation''' and '''recovering''' flows. Most of the '''dehibernation''' flow (aka ''restoreState'') is handled here, while the remainder is done in ''_waitForIncomingMigrationToFinish'', where  the '''migration destination''' flow is also implemented.

The ''_run'' method is surrounded by a BoundingSemaphore to throttle the access to libvirt. In the current implementation no more than 4 (four) parallel accesses to libvirt are permitted.
This affects the entire ''creation'' and ''recovery'' flow and partially the ''dehibernation'' flow, which is implemented partially in ''_waitForIncomingMigrationFInish''.

=== The VM Creation flow ===

This execution flow boot up a VM previously reported as Down in the data center. It is named ''creation'' by borrowing the libvirt jargon (the libvirt call being used is domain.CreateXML).
This is the only execution flow which do not assume a VM is already up and being handled by libvirt.
VDSM recreates the VM definition in the XML format used by libvirt from the data provided by engine, and feed it into libvirt. libvirt will do the heavy lifting with qemu/kvm.

The most important steps are:
* translation of the device data sent to engine in the internal data structure (''buildConfDevices'')
* normalization of devices and enforcing the device limits (''preparePaths'', ''_prepareTransientDisks'' et al. See point below)
* setup of the drive paths/images: oVirt uses shared storage and this has to be set before a VM can run; this is done by using the services provided by the VDSM storage subsystem
* translate the internal data representation into the libvirt XML format (''_buildCmdLine'')
* create the Domain (libvirt jargon) by using this XML, effectively starting up the VM
* perform post-creation domain checks (''_domDependentInit'', shared with the other flows): 
** resync data representation with the libvirt one (''_getUnderlyingVmInfo'')
** update and resync the device information from libvirt (''_getUnderlyingVmDevicesInfo'' and sub-methods) 
** start the statistics gathering thread, one per VM
** (try to) connect to Guest Agent
** handle paused VM, the most important task is handling VM paused for disk space exausted and handle this case appropriately
** set up niceness and guest scheduler parameters
* last but not least, run the hooks

This flows is composed to many steps but is may be the most striaghtforward because there is no state to be synchronized between parties. The engine has the reference state, VDSM is acting
as middlemen for libvirt and mostly translating data from the engine representation to the libvirt representation. Moreover, most of actions involved, and most of the helpers which implements them, are shared with the other execution flows.

=== The VM Recovery flow ===

This execution flow recovers a VM after a VDSM restart, crash or genera unavailability. VM running in an host should not be affected by VDSM restarts, and they should continue to run.
When VDSM returns up, it resyncs with the running VMs to regain the control and to be able again to manage them.

Please note that we document here just the part of the recovery which affects a VM startup. Recovery is a broader process and other parts of VDSM (clientIF) are affected.
The recovery flow is implemented in the ''_run'' method. VDSM uses the data saved by the ''saveState'' call to restore most of its internal state, and merges those informations with the data provided by libvirt.

=== The VM Dehibernation flow ===
Work in progress...

=== The VM Migration Destination flow ===
Work in progress...


== Rewrite objectives ==

* add more tests! '''both''' unit-tests and functional (probably need to revamp vm functional tests as well)

* make the code cleaner/more extensible

* make the code more robust

* avoid racy behaviour (see [https://bugzilla.redhat.com/show_bug.cgi?id=912390 bz912390])

== Rewrite proposals ==

=== Draft 1 ===

Meta-proposal: try to preserve orthogonality between the folling concepts; e.g. allow to drop the 'staging area' concept while preserving the 'separate control flow' concept. Avoid inter-dependent enhancements wherever feasible.

* clearly separate the control flows for each major startup mode (creation, recovery, restoring state); avoid multiplex-like functions like _run

* OK to create the VMs in a separate thread, throttle parallelism until we can fully depend on libvirt not being a bottleneck ([[VDSM_libvirt_performance_scalability#Startup_of_many_VMs|details here]])

* introduce 'staging area' for VMs being created, e.g. while the creation thread is running. In a nutshell, a separate private vmContainer-like structure to hold half-baked VMs; move VMs to public vmContainer -as it is now- only when they are fully created. Rationale: improve transactional behaviour as seen from the outside (engine), and make code more robust.
