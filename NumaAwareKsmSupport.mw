<!-- {{autolang|base=yes}} -->

<!-- The actual name of your feature page should look something like: "Your feature name". Use natural language to name the pages. -->

= NUMA aware KSM support <!-- The name of your feature --> =

{{Feature|name=NUMA aware KSM support|modules=engine, vdsm, MoM, GUI, REST|status=Design|version=3.6}}

== Summary ==
The KSM feature is optimizing shared memory pages across all NUMA nodes. The consequences is: a shared memory pages (controlled by KSM) to be read from many CPUs across NUMA nodes. Hence degrading (could totally eliminate) the NUMA performance gain.
The optimal behavior is for KSM feature to optimize memory usage per NUMA nodes. This feature is implemented in KSM since [https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Administration_Guide/chap-KSM.html RHEL 6.5].
We want to introduce a new feature to oVirt that will allow host administrator to control NUMA aware KSM policy.

== Owner ==
* Feature Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>
* GUI Component Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>
* Engine Component Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>
* VDSM Component Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>
* MoM Component Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>
* REST Component Owner: Dudi Maroshi <dmaroshi (at) redhat (dot) com>

== General Description ==
The information flow is presented in the diagram bellow:

[[File:Ksm-merge_across-node-data-flow.png]]<br />
The implementation activities are the following:<br />
# Update Database and entities with new attribute.
# Update REST api and its translation system with new attribute
# Update GUI with new widget 
# Test GUI and REST api
# Update oVirt-engine business logic to transmit KSM changes to host
# Test oVirt-engine
# Update MoM subsystem with new attribute
# Test MoM subsystem to track new attribute, and update KSM policy
# Update VDSM agent to update new MoM policy with new attribute

== Detailed Description ==
We want to introduce a new feature to HREV-M that will allow host administrator to control NUMA aware KSM.<br />


=== Kernel level solution ===
Since RHEL 6.5 there is a kernel flag that controls KSM's NUMA awareness. The flag '''/sys/kernel/mm/ksm/merge_across_nodes''' has strict logic for enabling/disabling NUMA awareness in KSM. Especially this documented lifecycle constraint:<br />

“''merge_across_nodes setting can be changed only when there are no ksm shared pages in system: set run 2 to unmerge pages first, then to 1 after changing merge_across_nodes, to remerge according to the new setting. Default = 1 (merging across NUMA nodes as in earlier releases)''”<br />

The initial life-cycle of KSM service with NUMA awareness is  presented in the pic bellow:<br />
[[File:ksm-merge-nodes-statechart.png]]<br />
The skilled hypervisor/host administrator may control merge_across_nodes life-cycle using scripts. Outside oVirt control.<br />
=== oVirt feature design ===
Add cluster level property to store and manage the NUMA aware KSM policy. <br />
This requires adding new boolean column to vds_group table. <br />
Updating the corresponding insert and update stored procedure. <br />
Refactoring the DAO command and refactoring VDSGroup class.<br />

Refactor all usage scenarios to guarantee that change to above cluster property will percolate at runtime to all active hosts. And NUMA aware KSM policy will be effective on every host activation. Using GUI and REST api.<br />

Eventually achieving NUMA aware KSM policy conformance on all hosts in cluster.
The initial default to all hosts in data center is merge_across_nodes = 1 (NUMA performance loss) this is RHEL default initial state.
==== A) Activate host with NUMA aware KSM policy ====
# Refactor engine class InitVdsOnUpCommand to send VDSM the current NUMA aware KSM policy value.<br />
# Refactor VDSM command SetMOMPolicyParameters to accept NUMA aware KSM policy. And make the VDSM side modification to receive the command. Preserving host performance. <br />
# On MoM update KSM controller and collector to identify new parameter. Also update the KSM policy to reflect the KSM with '''ksm_across_nodes''' flag life-cycle logic.<br />
==== B) Update cluster with NUMA aware KSM policy using REST api ====
# Refactor engine class UpdateVdsGroupCommand. At execution: concurrently distributed the NUMA aware KSM policy values to each active host (calling VDSM command SetMOMPolicyParameters as in section A above).<br />
# Update REST api.xsd rsdl file and mapping classes<br />
# On VDSM and MoM side use the design from section A above.
==== C) Update cluster with NUMA aware KSM policy using GUI ====
In addition to the design from section B) above. Add radio buttons to engine GUI  ClusterPopup at the bottom enabled by KSM checkbox.<br />
As in the picture bellow:<br />
[[File:KSM-Policy-for-Numa-GUI.png]]<br />
==== D) Adding new cluster with NUMA aware KSM policy using REST api ====
Refactor engine class AddVdsGroupCommand. Add validation to NUMA aware KSM policy value. 
==== E) Adding new cluster with NUMA aware KSM policy using GUI ====
Apply the GUI changes in section C above. And the design in section D above.

== Benefit to oVirt ==
# Optimize host performance, by optimizing NUMA performance with KSM memory saving.
# Effectively apply KSM policy to all active host at request time, not requiring maintenance-activation.

== Dependencies / Related Features ==
=== Data and entities ===
* Database – update vds_groups table (boolean, default =  true, not null)
* Database – update stored procedure insertvdsgroups
* Database – update stored procedure updatevdsgroups
* Database – Schema upgrade script.
* Class VdsGroupDAODbFacadeImpl - update
* Class VDSGroup – update
=== REST api === 
* File api.xsd - update
* File rsdl_metadata.yaml - update
* Class ClusterMapper – update
=== oVirt engine backend ===
* Class MomPolicyVDSParameters – update
* Class InitVdsOnUpCommand – update
* Class AddVdsGroupCommand – update
* Class UpdateVdsGroupCommad – update
* Class SetMOMPolicyParametersVDSCommand – update
* Class VDSProperties - update
=== oVirt engine GUI ===
* Class ClusterPopupView – update
* File clusterPopupView.ui.xml – update
* Class ClusterModel – update
=== VDSM === 
* File supervdsmServer – update 
* Class vdsm/momIF.py – update
=== MoM ===
* Class controller file KSM.py –  update
* Class collector file HostKSM.py –  update
* Policy file File KSM.rules -update
== Testing ==
Following are test scenarios for this feature.<br />
Required: A cluster with 2 NUMA capable hosts<br />

=== Activate host with NUMA aware KSM policy ===
This test can be performed with REST or GUI.<br />
'''Initial set up:'''<br />
Take all hosts in clusters on maintenance.<br />
Set cluster with with NUMA aware KSM policy = KSM share NUMA nodes (the default).<br />
==== Test 1 – system test ====
# Using oVirt engine, activate 1 host
# When host is up<br />
#:Verify: On host file /sys/kernel/mm/ksm/merge_across_nodes contains 1  .
# Take host 1 on maintenance
# When host is on maintenance
#:Update cluster with with NUMA aware KSM policy = KSM per each NUMA nodes.
# Using oVirt engine, activate 1 host
#When host is up
#: Verify: On host file /sys/kernel/mm/ksm/merge_across_nodes contains 0  .
#: Verify: received info event from host about  NUMA aware KSM policy change.
# Take host 1 on maintenance
#When host is on maintenance
#: Update cluster with with NUMA aware KSM policy = KSM share NUMA nodes.
# Using oVirt engine, activate 1 host
# When host is up
#: Verify: On host file /sys/kernel/mm/ksm/merge_across_nodes contains 1  .
#: Verify: received info event from host about  NUMA aware KSM policy change.
=== Update cluster with NUMA aware KSM policy using REST===
Initial set up:<br />
Set cluster with with NUMA aware KSM policy = KSM share NUMA nodes (the default).<br />
Take all hosts in clusters up.
==== Test 1 – system test ====
# Update cluster with KSM disabled and  NUMA aware KSM policy = KSM per each NUMA nodes
#: Wait for 20 secs
# Verify: received from each host info event about  NUMA aware KSM policy change.
# Verify: on each host /sys/kernel/mm/ksm/merge_across_nodes contains 0  .
==== Test 2 – system test ====
# Update cluster with KSM disabled and  NUMA aware KSM policy = KSM share NUMA nodes
#: Wait for 20 secs
# Verify: received from each host info event about  NUMA aware KSM policy change.
# Verify: on each host /sys/kernel/mm/ksm/merge_across_nodes contains 1

== Comments and Discussion ==

* No visible feedback about individual host NUMA awareness for KSM (not in RFE requirements)
* Host administrator's manual configuration for /sys/kernel/mm/ksm/merge_across_nodes kernel flag. Is effective till next MoM collection cycle (important for manually trials and testing).
* Reusing and dependent on existing oVirt engine mechanisms, and reusing VDSM host mechanisms.

* Refer to [[Talk:NUMA aware KSM support]] 

[[Category:Feature]]
[[Category:Template]]
