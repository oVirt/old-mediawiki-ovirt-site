<!-- {{autolang|base=yes}} -->

= Affinity Rules Enforcement Manager =

{{Feature|name=Affinity Rules Enforcement Manager|modules=engine|status=In Development|version=3.6.0}}

== Summary ==
A new engine internal manager that will enforce affinity rules. The manager will periodically query a list of VMs that break affinity rules, and will try to resolve the conflicts by migrating problematic VMs. One VM each period. Each cluster will have a separate task in the manager.<br />
The following picture, explains AR (Affinity Rules), before enforcement and after enforcement.
[[File:Affinity_Rule_Enforcement.png]]

== Owner ==
http://www.ovirt.org/User:Tsaban

* Name: [[User:Tsaban| Tomer Saban]]
* Email: <tsaban@redhat.com>

== Procedure ==
[1][2][3]<br />
[[File:ARES_Life_Cycle.png]]
<br />
''' Trigger each time a cluster is created(Or for the default cluster at startup) '''
# Create new AffinityRulesEnforcementPerCluster and add to perClusterList.
# Set manager interval to “regular interval”.
# AreClusterIterator = perClusterList.iterator().
<br />
''' Trigger each time a cluster is deleted '''
# In AffintiyRulesEnforcementManager, find the instance associated with the cluster and delete it.
<br />
''' Trigger before manager shutdown '''
# Write AuditLog message: “Affinity Rules Enforcement Manager finished. all Affinity rules are enforced.”
<br />
''' Trigger each time a Create/Update/Delete action occur on affinity group and on engine startup '''
# lastMigrations = new List<MigrationEntryDS>{}.
# migrationTries = 0.
<br />
''' Manager wakes up '''
# migrationTries = 0.
# lastMigrations = new List<MigrationEntryDS>{}.
# Change interval to “regular interval” minute. Call the “Enforce affinity rule using migration”.
# Write AuditLog message: “Affinity Rules Enforcement Manager started”
<br />
[The following method identify broken affinity rule,  designate vm that breaks the rule and migrates the vm.]
<br />
''' Manager interval reached: Enforce affinity rules using migration '''
# if lastMigrations tail is currently migrating(Using PendingResourceManager):
## currentInterval = regular interval
## return.
# Else:
## if lastMigrations tail exists(not null):
### if lastMigrations tail.getMigrationStatus() is failure:
#### migrationTries++
#### lastMigrations tail.setMigrationReturnValue to null.
# if migrationTries == MAXIMUM_MIGRATION_TRIES:
## currentInterval = long interval
## migrationTries = 0.
## return.
# Else if currentInterval == Long interval:
## currentInterval = regular interval.
## migrationTries = 0.
# if AreClusterIterator.empty():
## AreClusterIterator = perClusterList.iterator();

# do
## cluster = AreClusterIterator.getNext();
## vm = Choose next vm to migrate(cluster).
# while vm is null and not AreClusterIterator.empty().
# if vm is null: (AreClusterIterator is empty)
## return. (All clusters are enforced. Returning).
# [4]if vm is not null:
## lastMigrations tail.setMigrationReturnValue = [5]Call scheduler to migrate vm.
## currentInterval = regular interval
# currentInterval = long interval (All affinity rules enforced).
# return.
<br />

== Related methods ==
''' Choose next vm to migrate(cluster) '''
# Get all affinity groups
# Get unified affinity groups().
: The following picture explains UAG (Unified Affinity Group) algorithm
: [[File:UAG_Algorithm.png]]
# [6]Sort all groups first by size and then by first VM UUID.
# Loop over all unified affinity groups:
## [7]candidate_host = choose_candidate_host_for_migration(Unified Affinity Group).
## loop all VMs in group:
### if current vm’s host is not candidate_host and [8]vm not in error state:
#### current_migration = MigrationEntryDS(current_vm, current_vm.runOnVds()).
#### If lastMigrations.contains(current_migration.opposite_migration()) or lastMigrations.contains(current_migration)
##### print “Migrations loop occurred. Shutting down manager.”
##### Shutdown manager.
#### Else:
##### lastMigrations.add(current_migration).
##### return current_migration.
::::: The following picture explains Migration loop occurrence and detection
::::: [[File:Migration_Loop_Detection.png]]
<br />

''' choose_candidate_host_for_migration(Unified Affinity Group - UAG) '''
# hosts = create_map_of_hosts_and_vms()
# Return hosts with max number of Vms on it(More Vms from the same UAG means less migrations needed to enforce the UAG).
<br />
''' create_map_of_hosts_and_vms() '''
# map = {}
# for AG in UAG:
## for VM in AG:
### host = null
### if vm in the middle of migration:
#### [9]get host that vm is migrating to.
### Else if Vm’s host is available:
#### get the host.
### if not host == null:
#### if host is key in map:
##### map[host]++
#### else:
##### map[host] = 1
# return map.

''' Get unified affinity groups() '''
# UAG = {{vm} for each vm}
# For each (+) affinity group(Sorted by group id):
## unify VMs from the group in UAG(Sorted by vm id).
## For each (-) affinity group(Sorted by group id):
### [10]For each group in UAG(Sorted by first vm uuid):
#### if size of the intersection of group from UAG and the negative group is > 1:
##### throw exception “Affinity group contradiction detected” (With associated groups).

''' MigrationEntryDS.opposite_migration(entry) '''
# [11]migration_entry_ds = MigrationEntryDS(entry.getCurrentVm, entry.getTargetHost())
#migration_entry_ds.setTargetHost(entry.getSourceHost())

''' MigrationEntryDS constructor(vm, sourceHost) '''
#this.vm = current_vm
#this.sourceHost = sourceHost

''' MigrationEntryDS setMigrationReturnValue(returnValue) ''' 
# this.migrationReturnValue = returnValue.

''' MigrationEntryDS getMigrationStatus() '''
# if this.migrationReturnValue not null:
## return this.migrationReturnValue.status
# else:
## return null

''' MigrationEntryDS getTargetHost() '''
# if this.targetHost not null:
##return this.targetHost
# else:
## return null


== Footnotes ==
[1]Methods are written in the section “related methods”.
<br />
[2] If no class is written(As for most methods/fields) assume it’s in the manager itself.
<br />
[3] Procedure here happens for every cluster separately.
<br />
[4] if no vm is found for migration, In this situation we decide to make the manager sleep for long interval because, affinity groups are enforced at that point.
<br />
[5] The migration command will be done automatically to take into account other policies that might be in use.
<br />
[6] It’s important to keep getting the same groups in the same order each time. That’s why we will use SortedSet. The order is kept because affinity group are not supposed to change during migrations and if an affinity group will change then it can be fixed by emptying the last migrations list(thus temporarily allowing opposite migration and preventing the manager from shutting down unnecessarily).
<br />
[7] candidate host is the host we think is going to be the host that the next vm should migrate to. But, we are using the scheduler’s automatic migration thus the scheduler may decide to migrate the vm somewhere else.
<br />
[8] Vms in error state can’t migrate.
<br />
[9] The migration might fail. But, because we don’t want to cause migration loop we prefer to assume that the migration will succeed when we decide which host should be the one that the vms will migrate to.
<br />
[10] VMs that don’t belong to any positive affinity group(In the UAG - Unified Affinity group). Still has group of their own. So, in the next step that group still need to be on a separate host(It is still possible to put two positive affinity groups on the same host but we let the scheduler decide that for us).
<br />
[11] This if statement checks that a migration in the opposite direction doesn’t occur(Replacing target host with source host in the last migration).
<br />

== Related Data Structures ==
''' MigrationEntryDS ''' 
# UUID vm.
# UUID sourceHost.
# vdcReturnValue migrationReturnValue.
# VDS targetHost

''' AffinityRulesEnforcementPerCluster '''
# List<MigrationEntryDS> lastMigrations.
# Integer currentInterval, migrationTries.

''' AffinityRulesEnforcementManager '''
# SHORT_INTERVAL = 1.
# LONG_INTERVAL = 15.
# MAXIMUM_MIGRATION_TRIES = 5.
# List<AffinityRulesEnforcemenetPerCluster> perClusterList.
# Iterator<AffinityRulesEnforcemenetPerCluster> AreClusterIterator;

<br />
== Testing ==
# Manager creation when cluster is created:
## Manager creation on startup.
### Start engine
### Check log to see that ARES has created PerCluster object for the default cluster.
## Manager creation on new cluster.
### Create new cluster
### Check log to see that ARES has created PerCluster object for the new cluster.
## Manager deletion of deleted cluster(pre-condition = test 1b):
### Delete cluster
### Check log to see that ARES has deleted PerCluster object for the deleted cluster.
# Manager interval check:
## Check regular interval:
### Create new cluster. 
### Check log to see that ARES has created PerCluster object for cluster. (Make sure cluster has affinity rules).
### Add 2 VMs on the same host
### Add negative affinity rule for the 2 VMs.
### wait for “regular interval”
### check that ARES interval reached.
## Check long interval: (Preconditions = one datacenter is up, affinity rules enforced).
### Create new cluster. 
### Check log to see that ARES has created PerCluster object for cluster. (Make sure cluster has no affinity rules).
### Wait for long interval
### Check logs to see that long interval reached.
# Manager Affinity Rules enforcement:
## Enforcement for positive affinity rule: (Precondition load balancer if off, All vms can run together on the same hypervisor)
### Start engine
### Add 2 hosts.
### For host A add 3 Vms
### For host B add 1 Vm
### Add positive affinity rule for all VMs.
### Wait for regular interval.
### See that the Vm from Host B migrated to host A.
## Enforcement for negative affinity rule: (Preconditions: load balancer if off, All vms can run together on the each hypervisor)
### Start engine
### Add 2 hosts.
### For host A add 2 Vms
### For host B no Vms should be running on it.
### Add negative affinity rule for the two VMs.
### Wait for regular interval.
### See that one Vm migrated from host A to host B.
## Enforcement for balanced hypervisors:(Precondition: All Vms should be able to run on the same hypervisor at once)
### Start engine
### Add 2 hosts
### For host A add 3 Vms
### For host B add 3 Vms
### Add positive affinity rule for all Vms.
### Wait for 6 regular intervals(6 * regular interval)
### See that all Vms migrated from one of the hosts to the other one.
# Affinity Rule contradictions check:
## Check two opposite conditions:
### Start engine
### Add 2 hosts
### Add 2 Vms
### Add positive affinity rule for both VMs.
### Add negative affinity rule for both VMs.
### Wait for regular interval.
### Check logs to see a new exception was thrown saying: “Affinity group contradiction detected” (With associated groups).
# Balancer competing managers:
## Check power saving and colliding affinity:
### Start engine
### Add 2 hosts
### Add 6 Vms(Each host should have 3 VMs).
### Add positive affinity rule for 3 VMs
### Put balancer on power saving.
### Wait for a 4 regular intervals(4 * regular interval).
### Check in the logs that the manager has been shutdown because of contradicting migrations.

== Benefit to oVirt ==
Affinity group will be better enforced when a manager will be checking them periodically.

== Dependencies / Related Features ==
#SchedulerUtilQuartzImpl - scheduleAFixedDelayJob, scheduleAOneTimeJob.
#VmAffinityFilterPolicyUnit - getAcceptableHosts, filter. 
#MigrateVmCommand.
#AddAffinityGroupCommand.
#SchedulerUtilQuartzImpl.

== Intervals and values used in the system ==
#Regular interval - 1 minute.
#Long interval - 15 minutes.
#Maximum migration tries - 5.
<br />

== Release Notes ==
This feature is a manager that checks if any affinity rules are broken and migrates VMs in order to enforce them. 
<br />
This manager includes:
# Triggers for creation of the manager on Cluster Creation/Deletion.
# Managers wakes up on Affinity group Creation/Deletion/Update and engine startup.
# Manager Sleeps long interval when too many migration tries fail.
# Manager shutsdown when migration loop is detected or if the same migration happens more than once on the last 5 migrations.
# Manager avoid multiple migrations on the same cluster by sleeping when migration in progress(Using pending resource manager).
# Manager uses scheduler's automatic migration command to comply with filter and weight policies.
# Manager will sleep for 15 minutes instead of 5 if all affinity rules are enforced when interval is reached.
# Manager will automatically sort and create "Unified Affinity Rules" based on the defined affinity rules and will detect contradictions between rules.
# Manager will enforce affinity rules one by one in order to reduce the number of broken affinity rules as soon as possible.
# Manager will try to resolve a specific affinity rule break by migrating VMs from hosts with less VMs from a than affinity group to the one with the maximum VMs.

== Phase 2 ==
# Using UAG algorithm to tell the user where there are conflicting affinity rules and also if the affinity rules can be optimized by uniting positive intersecting groups.
# Taking into consideration host's RAM, CPU type, Network interfaces etc in order choose the best host to migrate the affinity group to.
# Taking into consideration where a vm might be migrated(This is supported in the system already but not in this feature). That way the enforcement process can be optimized better.

== Comments and Discussion ==
For more information see the following BugZilla link:
<br />
https://bugzilla.redhat.com/show_bug.cgi?id=1112332

* Refer to [[Talk:Affinity_Group_Enforcement_Manager]]  <!-- This adds a link to the "discussion" tab associated with your page.  This provides the ability to have ongoing comments or conversation without bogging down the main feature page -->

[[Category:Feature|Affinity Group Enforcement Manager]]
[[Category:oVirt 3.6 Proposed Feature|Affinity Group Enforcement Manager]]
[[Category:SLA]]
