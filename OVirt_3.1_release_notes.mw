The oVirt Project is pleased to announce the availability of its second formal release, oVirt 3.1.

= What's New? =

The oVirt 3.1 release includes these notable changes.

== Installer ==

* The installation script now supports the configuration of a HTTP/HTTPS proxy, allowing the oVirt Engine to be accessed via port 80 for HTTP, and port 443 for HTTPS ([[Features/OvirtEnginePort80]]).
* The oVirt Engine now supports the use of a remote PostgreSQL database server, specified during installation ([[Features/RemoteDB]]). 
* An 'all-in-one' proof of concept mode is now available. This allows a single machine to both run the management engine and act as a virtualization host ([[Feature/AllInOne]]).

== Tools ==

* The Log Collector (engine-log-collector), ISO Uploader (engine-iso-uploader), and Image Uploader (engine-image-uploader) have been rebased to access the oVirt Engine using the new Python SDK. Previously these tools, written in Python, accessed the REST API directly.

== Infrastructure ==

* The oVirt Engine is now supports the use of Red Hat Directory Server and/or IBM Tivoli Directory Server for user authentication. This is addition to existing support for IPA and Active Directory.
* An additional tab has been added to the oVirt Engine's management interface to support monitoring the status of tasks in the oVirt Environment.
* A correlation identifier to support debugging is now used to track events across the user interfaces, engine backend, and VDSM. 
* The oVirt Engine now automatically attempts to auto-activate hosts detected as non-operational.

== User Interface ==

* Infrastructure supporting localization has been added, with translations to follow in a later release.
* Infrastructure supporting integration of reports functionality has been added to the oVirt Engine management interface.

== Storage ==

* Support has been added for presenting any block device as a local disk attached to a virtual machine simply by specifying the device's GUID. This provides '''directlun''' support, which previously had to be implemented using a VDSM hook ([[Features/Direct_Lun]]).
* Support has been added to VDSM, and by extension oVirt Engine, for the attachment and use of NFSv4 storage ([[Features/NFSv4]].
* It is now possible to override some of VDSM's default NFS settings from the oVirt Engine when attaching storage ([[Features/AdvancedNfsOptions]]).
* Support has been added for the attachment and use of POSIX filesystem compliant storage, allowing users to attach any type of storage supported by '''mount''' ([[Features/PosixFSConnection]]).
* Support for defining the priority of hosts in the storage pool manager (SPM) selection process has been added. Hosts can also be assigned a priority of '''-1'''m which means that they must not be selected as the SPM ([[Features/SPMPriority]]).
* Support has been added for sharing of disks between virtual machines. Previously each disk could only be attached to a single virtual machine, it is now possible to share a disk between multiple virtual machines concurrently ([[Features/SharedRawDisk]]).
* Support has been added for hot plugging and unplugging of '''virtio-blk''' disks to and from virtual machines If the "Add" menu command in the virtual disks pane of a running VM is disabled, a disk may be added from the VM "Guide Me" dialog) ([[Features/HotPlug]]).
* Support has been added for floating disks, disks which are not necessarily attached to a virtual machine at any one point in time but can be attached to virtual machines as and when needed. Floating disks can be found in the web administration portal under the '''Disks''' tab ([[Features/FloatingDisk]]).
* It is now possible for virtual machines to have their disks spread across multiple storage domains within the same data center. Previously all disks attached to a virtual machine had to reside on the same storage domain ([[Features/MultipleStorageDomains]]). 
* Live snapshotting of virtual machines running on virtualization hosts with QEMU 1.1 or higher is now supported ([[Live_Snapshots]]).

<!-- Didn't get confirmation that this is in 3.1 
* '''use sanlock for pool locks (need to see if made ovirt 3.1)'''-->

== Virtualization ==

* Previously it was not possible to import a Virtual Machine or Template that had already been imported, even if it was to a different data center. This is no longer the case ([[Features/ImportMoreThanOnce]]).
* Devices attached to  guest virtual machines now retain the same address allocations as other devices are added and/or removed from the guest's configuration ([[Features/Design/DetailedStableDeviceAddresses]]).
* Support for "pre-started" virtual machine pools has been added. Instead of having to wait for a virtual machine allocated from the pool to boot user's will instead, where possible, be allocated a virtual machine which has already been started ([[Features/PrestartedVm]]).
* Support for virtual machine payloads has been added, in the form of a Floppy or CD/DVD image that will be passed to the virtual machine. Virtual machine payloads may be either temporary or permenant.([[Features/VMPayload]]).
* Added support for cloning a Virtual Machine from a specific Snapshot ([[Features/CloneVmFromSnapshot]]).
* Support has been added for virtualization hosts with Intel Sandybridge and Opteron G4 based CPUs.

<!-- DIDN'T GET REQUIRED INFO, NO FEATURE PAGE FOUND:
* new custom hook in vdsm: set vm ticket
* '''vnc details screen (need to see if made ovirt 3.1)'''-->

== SLA ==

* Added support for the definition quotas restricting user resource usage ([[Features/Quota]]).
* Added support for pinning Virtual Machines to specific physical CPUs ([[Features/Design/cpu-pinning]]).

== Network ==

* A new network setup API to support complex network provisioning tasks has been added to the backend, as a result the user interface for host network setup has also been redesigned  ([[Features/Design/Network/SetupNetworks]]).
* It is now possible to adjust the MTU of a logical network, when it is not attached to a cluster ([[Features/Design/Network/Jumbo_frames]]).
* It is now possible to create bridgeless logical networks, previously all logical network were represented using a bridge ([[Features/Design/Network/Bridgeless_Networks]]).
* Hot plugging and unplugging of virtual Network Interface Cards from virtual machines is now supported ([[Features/HotplugNic]]).
* Support for port mirroring, allowing all network traffic to be mirrored to a specific virtual machine, has been added ([[Features/PortMirroring]]).
* Previously, all logical networks were considered mandatory for all hosts in a cluster. Hosts that were not attached to all logcal networks in the cluster were marked non-responsive. Administrators now have the option to mark specific logical networks as non-mandatory, bypassing this behavior ([[Features/Design/Network/Required_Networks]]).

== Interfaces ==

* New Python SDK, packaged as ''ovirt-engine-sdk'' ([[SDK]]).
* New Python CLI, packaged as ''ovirt-engine-cli'' ([[CLI]]).
* Added JSON support for REST API.
* Added session support to the REST API, allowing one login to process multiple requests.

= Installation Instructions =

== oVirt Engine ==

The oVirt Engine provides the browser based management interface for managing your oVirt environment. It also provides command line tools for managing configuration options not exposed via the user interface as well as a series of APIs supporting automation of both common and advanced tasks.

=== Fedora ===

To install the oVirt Engine on a Fedora 17 system:

* Log in to the system on which you wish to host oVirt Engine as the '''root''' user.
* Install the ''ovirt-release'' package using '''yum''', this package configures your system to receive updates from the oVirt project's software repository:

    # yum localinstall http://ovirt.org/releases/ovirt-release-fedora.noarch.rpm

* Install the ''ovirt-engine'' package, and all of the packages it depends on, using '''yum''':

    # yum install ovirt-engine

* Run the '''engine-setup''' script and follow the prompts to complete installation of oVirt Engine. Once the Engine has been installed successfully the script will provide instructions for accessing the web Administration Portal:

    # engine-setup

Suggested quick start path for new users:

* Add a host to the ''Default'' cluster.
* Add storage, refer to [[Troubleshooting_NFS_Storage_Issues]] for information on configuring NFS storage for use with oVirt. Support is also available for iSCSI, FCP, and POSIX compliant filesystem storage.
** Add a data storage domain to the ''Default'' data center.
** Add an ISO storage domain to the ''Default'' data center.
* Upload operating system installation media, in ISO format, to the ISO storage domain using the '''engine-iso-uploader''' command line tool.
    # engine-iso-uploader -i MyISODomainName /root/Downloads/Fedora-17-x86_64-DVD.iso
* Create a virtual machine!

== oVirt Node ==

= Upgrade Instructions =

Upgrading from oVirt 3.0 to oVirt 3.1 is '''not''' recommended. Users are instead advised to use the migration process as documented on this page. Upgrade instructions are however available, refer to [[OVirt_3.0_to_3.1_upgrade]] for more information.

= Migration Instructions =

Users of oVirt 3.0 who wish to migrate to oVirt 3.1 should:

* Create an export storage domain.
* Export all virtual machines to the export storage domain.
* Detach the export storage domain.
* Cleanup the engine installation:
    # engine-cleanup
    # yum remove ovirt\* 
* Upgrade to Fedora 17: https://fedoraproject.org/wiki/How_to_use_PreUpgrade
* Install oVirt Engine 3.1.
* Attach the export storage domain.
* Import all virtual machines.
